{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-89-232.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffbb4773b10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "import nltk\n",
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('xxxx')\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary.\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = sc.wholeTextFiles(\"s3a://zihe-public/articles/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_title(x):\n",
    "    pattern = \"\\<doc\\sid\\=\\\"(\\d+)\\\"(.*)title\\=\\\"(.*)\\\"\\>\"\n",
    "    pattern_re = re.compile(pattern)\n",
    "    matches = pattern_re.search(x)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    return matches[3]\n",
    "\n",
    "def get_content(x):\n",
    "    pattern = \"\\<doc\\sid\\=\\\"(\\d+)\\\"(.*)title\\=\\\"(.*)\\\"\\>\\\\n(.*?)\\\\n{2}\"\n",
    "    pattern_re = re.compile(pattern)\n",
    "    matches = pattern_re.search(x)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    idx = matches.end(0)\n",
    "    return x[idx:]\n",
    "\n",
    "def get_id(x):\n",
    "    pattern = \"\\<doc\\sid\\=\\\"(\\d+)\\\"(.*)title\\=\\\"(.*)\\\"\\>\"\n",
    "    pattern_re = re.compile(pattern)\n",
    "    matches = pattern_re.search(x)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    return matches[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|               title|                text|\n",
      "+---+--------------------+--------------------+\n",
      "| 12|           Anarchism|Anarchism is a po...|\n",
      "| 25|              Autism|Autism is a devel...|\n",
      "| 39|              Albedo|Albedo () (, mean...|\n",
      "|290|                   A|A or a is the fir...|\n",
      "|303|             Alabama|Alabama () is a s...|\n",
      "|305|            Achilles|In Greek mytholog...|\n",
      "|307|     Abraham Lincoln|Abraham Lincoln (...|\n",
      "|308|           Aristotle|Aristotle (; \"Ari...|\n",
      "|309|An American in Paris|An American in Pa...|\n",
      "|316|Academy Award for...|The Academy Award...|\n",
      "|324|      Academy Awards|The Academy Award...|\n",
      "|330|             Actrius|Actresses (Catala...|\n",
      "|332|     Animalia (book)|Animalia is an il...|\n",
      "|334|International Ato...|International Ato...|\n",
      "|336|            Altruism|Altruism is the p...|\n",
      "|339|            Ayn Rand|Ayn Rand (; born ...|\n",
      "|340|        Alain Connes|Alain Connes (; b...|\n",
      "|344|          Allan Dwan|Allan Dwan (3 Apr...|\n",
      "|358|             Algeria|Algeria ( ), offi...|\n",
      "|359|List of Atlas Shr...|This is a list of...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = wiki.flatMap(lambda x: (x[1].split('</doc>'))).map(lambda x : (get_id(x), get_title(x), get_content(x)))\n",
    "text = text.toDF([\"id\",\"title\",\"text\"])\n",
    "text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|               title|                text|            document|               token|          normalized|               lemma|         clean_lemma|finished_clean_lemma|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 12|           Anarchism|Anarchism is a po...|[[document, 0, 34...|[[token, 0, 8, An...|[[token, 0, 8, an...|[[token, 0, 8, an...|[[token, 0, 8, an...|[anarchism, polit...|\n",
      "| 25|              Autism|Autism is a devel...|[[document, 0, 43...|[[token, 0, 5, Au...|[[token, 0, 5, au...|[[token, 0, 5, au...|[[token, 0, 5, au...|[autism, developm...|\n",
      "| 39|              Albedo|Albedo () (, mean...|[[document, 0, 17...|[[token, 0, 5, Al...|[[token, 0, 5, al...|[[token, 0, 5, al...|[[token, 0, 5, al...|[albedo, mean, wh...|\n",
      "|290|                   A|A or a is the fir...|[[document, 0, 66...|[[token, 0, 0, A,...|[[token, 0, 0, a,...|[[token, 0, 0, a,...|[[token, 14, 18, ...|[first, letter, f...|\n",
      "|303|             Alabama|Alabama () is a s...|[[document, 0, 72...|[[token, 0, 6, Al...|[[token, 0, 6, al...|[[token, 0, 6, al...|[[token, 0, 6, al...|[alabama, state, ...|\n",
      "|305|            Achilles|In Greek mytholog...|[[document, 0, 33...|[[token, 0, 1, In...|[[token, 0, 1, in...|[[token, 0, 1, in...|[[token, 3, 7, gr...|[greek, mythology...|\n",
      "|307|     Abraham Lincoln|Abraham Lincoln (...|[[document, 0, 76...|[[token, 0, 6, Ab...|[[token, 0, 6, ab...|[[token, 0, 6, ab...|[[token, 0, 6, ab...|[abraham, lincoln...|\n",
      "|308|           Aristotle|Aristotle (; \"Ari...|[[document, 0, 55...|[[token, 0, 8, Ar...|[[token, 0, 8, ar...|[[token, 0, 8, ar...|[[token, 0, 8, ar...|[aristotle, arist...|\n",
      "|309|An American in Paris|An American in Pa...|[[document, 0, 11...|[[token, 0, 1, An...|[[token, 0, 1, an...|[[token, 0, 1, an...|[[token, 3, 10, a...|[american, paris,...|\n",
      "|316|Academy Award for...|The Academy Award...|[[document, 0, 75...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[token, 0, 2, th...|[[token, 4, 10, a...|[academy, award, ...|\n",
      "|324|      Academy Awards|The Academy Award...|[[document, 0, 39...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[token, 0, 2, th...|[[token, 4, 10, a...|[academy, award, ...|\n",
      "|330|             Actrius|Actresses (Catala...|[[document, 0, 18...|[[token, 0, 8, Ac...|[[token, 0, 8, ac...|[[token, 0, 8, ac...|[[token, 0, 8, ac...|[actress, catalan...|\n",
      "|332|     Animalia (book)|Animalia is an il...|[[document, 0, 20...|[[token, 0, 7, An...|[[token, 0, 7, an...|[[token, 0, 7, an...|[[token, 0, 7, an...|[animalia, illust...|\n",
      "|334|International Ato...|International Ato...|[[document, 0, 67...|[[token, 0, 12, I...|[[token, 0, 12, i...|[[token, 0, 12, i...|[[token, 0, 12, i...|[international, a...|\n",
      "|336|            Altruism|Altruism is the p...|[[document, 0, 32...|[[token, 0, 7, Al...|[[token, 0, 7, al...|[[token, 0, 7, al...|[[token, 0, 7, al...|[altruism, princi...|\n",
      "|339|            Ayn Rand|Ayn Rand (; born ...|[[document, 0, 40...|[[token, 0, 2, Ay...|[[token, 0, 2, ay...|[[token, 0, 2, ay...|[[token, 0, 2, ay...|[ayn, rand, bear,...|\n",
      "|340|        Alain Connes|Alain Connes (; b...|[[document, 0, 98...|[[token, 0, 4, Al...|[[token, 0, 4, al...|[[token, 0, 4, al...|[[token, 0, 4, al...|[alain, connes, b...|\n",
      "|344|          Allan Dwan|Allan Dwan (3 Apr...|[[document, 0, 31...|[[token, 0, 4, Al...|[[token, 0, 4, al...|[[token, 0, 4, al...|[[token, 0, 4, al...|[allan, dwan, apr...|\n",
      "|358|             Algeria|Algeria ( ), offi...|[[document, 0, 61...|[[token, 0, 6, Al...|[[token, 0, 6, al...|[[token, 0, 6, al...|[[token, 0, 6, al...|[algeria, officia...|\n",
      "|359|List of Atlas Shr...|This is a list of...|[[document, 0, 83...|[[token, 0, 3, Th...|[[token, 0, 3, th...|[[token, 0, 3, th...|[[token, 10, 13, ...|[list, character,...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned = pipeline.fit(text).transform(text)\n",
    "cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rawData = sc.textFile(\"s3://zhiyu-lin-first-s3/dataset.tsv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kecilRawData = rawData.map(lambda x: x.lower())\n",
    "fields = kecilRawData.map(lambda x: x.split(\"\\t\"))\n",
    "documents = fields.map(lambda x: x[2].split(\" \"))\n",
    "documentId = fields.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "documents.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = cleaned.select('finished_clean_lemma').rdd.flatMap(list)\n",
    "documentId = cleaned.select('id').rdd.flatMap(list)\n",
    "\n",
    "hashingTF = HashingTF(100000)\n",
    "tf = hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cache()\n",
    "idf = IDF(minDocFreq=1).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizedtfidf=normalizer.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"mathematics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordTF = hashingTF.transform([keyword.lower()])\n",
    "keywordHashValue = int(keywordTF.indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordRelevance = normalizedtfidf.map(lambda x: x[keywordHashValue])\n",
    "zippedResults = keywordRelevance.zip(documentId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010993429132272574,\n",
       " 0.005807206658798934,\n",
       " 0.005286009919697647,\n",
       " 0.004883073523233428,\n",
       " 0.003461760271393371,\n",
       " 0.003270489891544955,\n",
       " 0.003161939544531214,\n",
       " 0.003029951350310274,\n",
       " 0.002756716441453091,\n",
       " 0.0026975349989878774]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywordRelevance.top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.010993429132272574, '334')\n"
     ]
    }
   ],
   "source": [
    "print(zippedResults.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|title                    |\n",
      "+-------------------------+\n",
      "|International Atomic Time|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned.createOrReplaceTempView(\"table_df\")\n",
    "out = sqlContext.sql(\"\"\"SELECT title FROM table_df WHERE id == 334\"\"\")\n",
    "out.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
