{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-89-232.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-nlp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efe4001bb90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "import nltk\n",
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('xxxx')\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary.\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = sc.wholeTextFiles(\"s3a://zihe-public/articles/AA/wiki_00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_title(x):\n",
    "    pattern = \"\\<doc\\sid\\=\\\"(\\d+)\\\"(.*)title\\=\\\"(.*)\\\"\\>\"\n",
    "    pattern_re = re.compile(pattern)\n",
    "    matches = pattern_re.search(x)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    return matches[3]\n",
    "\n",
    "def get_content(x):\n",
    "    pattern = \"\\<doc\\sid\\=\\\"(\\d+)\\\"(.*)title\\=\\\"(.*)\\\"\\>\\\\n(.*?)\\\\n{2}\"\n",
    "    pattern_re = re.compile(pattern)\n",
    "    matches = pattern_re.search(x)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    idx = matches.end(0)\n",
    "    return x[idx:]\n",
    "\n",
    "def get_id(x):\n",
    "    pattern = \"\\<doc\\sid\\=\\\"(\\d+)\\\"(.*)title\\=\\\"(.*)\\\"\\>\"\n",
    "    pattern_re = re.compile(pattern)\n",
    "    matches = pattern_re.search(x)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    return matches[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|               title|                text|\n",
      "+---+--------------------+--------------------+\n",
      "| 12|           Anarchism|Anarchism is a po...|\n",
      "| 25|              Autism|Autism is a devel...|\n",
      "| 39|              Albedo|Albedo () (, mean...|\n",
      "|290|                   A|A or a is the fir...|\n",
      "|303|             Alabama|Alabama () is a s...|\n",
      "|305|            Achilles|In Greek mytholog...|\n",
      "|307|     Abraham Lincoln|Abraham Lincoln (...|\n",
      "|308|           Aristotle|Aristotle (; \"Ari...|\n",
      "|309|An American in Paris|An American in Pa...|\n",
      "|316|Academy Award for...|The Academy Award...|\n",
      "|324|      Academy Awards|The Academy Award...|\n",
      "|330|             Actrius|Actresses (Catala...|\n",
      "|332|     Animalia (book)|Animalia is an il...|\n",
      "|334|International Ato...|International Ato...|\n",
      "|336|            Altruism|Altruism is the p...|\n",
      "|339|            Ayn Rand|Ayn Rand (; born ...|\n",
      "|340|        Alain Connes|Alain Connes (; b...|\n",
      "|344|          Allan Dwan|Allan Dwan (3 Apr...|\n",
      "|358|             Algeria|Algeria ( ), offi...|\n",
      "|359|List of Atlas Shr...|This is a list of...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = wiki.flatMap(lambda x: (x[1].split('</doc>'))).map(lambda x : (get_id(x), get_title(x), get_content(x)))\n",
    "text = text.toDF([\"id\",\"title\",\"text\"])\n",
    "text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|               title|                text|            document|               token|          normalized|               lemma|         clean_lemma|finished_clean_lemma|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 12|           Anarchism|Anarchism is a po...|[[document, 0, 34...|[[token, 0, 8, An...|[[token, 0, 8, an...|[[token, 0, 8, an...|[[token, 0, 8, an...|[anarchism, polit...|\n",
      "| 25|              Autism|Autism is a devel...|[[document, 0, 43...|[[token, 0, 5, Au...|[[token, 0, 5, au...|[[token, 0, 5, au...|[[token, 0, 5, au...|[autism, developm...|\n",
      "| 39|              Albedo|Albedo () (, mean...|[[document, 0, 17...|[[token, 0, 5, Al...|[[token, 0, 5, al...|[[token, 0, 5, al...|[[token, 0, 5, al...|[albedo, mean, wh...|\n",
      "|290|                   A|A or a is the fir...|[[document, 0, 66...|[[token, 0, 0, A,...|[[token, 0, 0, a,...|[[token, 0, 0, a,...|[[token, 14, 18, ...|[first, letter, f...|\n",
      "|303|             Alabama|Alabama () is a s...|[[document, 0, 72...|[[token, 0, 6, Al...|[[token, 0, 6, al...|[[token, 0, 6, al...|[[token, 0, 6, al...|[alabama, state, ...|\n",
      "|305|            Achilles|In Greek mytholog...|[[document, 0, 33...|[[token, 0, 1, In...|[[token, 0, 1, in...|[[token, 0, 1, in...|[[token, 3, 7, gr...|[greek, mythology...|\n",
      "|307|     Abraham Lincoln|Abraham Lincoln (...|[[document, 0, 76...|[[token, 0, 6, Ab...|[[token, 0, 6, ab...|[[token, 0, 6, ab...|[[token, 0, 6, ab...|[abraham, lincoln...|\n",
      "|308|           Aristotle|Aristotle (; \"Ari...|[[document, 0, 55...|[[token, 0, 8, Ar...|[[token, 0, 8, ar...|[[token, 0, 8, ar...|[[token, 0, 8, ar...|[aristotle, arist...|\n",
      "|309|An American in Paris|An American in Pa...|[[document, 0, 11...|[[token, 0, 1, An...|[[token, 0, 1, an...|[[token, 0, 1, an...|[[token, 3, 10, a...|[american, paris,...|\n",
      "|316|Academy Award for...|The Academy Award...|[[document, 0, 75...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[token, 0, 2, th...|[[token, 4, 10, a...|[academy, award, ...|\n",
      "|324|      Academy Awards|The Academy Award...|[[document, 0, 39...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[token, 0, 2, th...|[[token, 4, 10, a...|[academy, award, ...|\n",
      "|330|             Actrius|Actresses (Catala...|[[document, 0, 18...|[[token, 0, 8, Ac...|[[token, 0, 8, ac...|[[token, 0, 8, ac...|[[token, 0, 8, ac...|[actress, catalan...|\n",
      "|332|     Animalia (book)|Animalia is an il...|[[document, 0, 20...|[[token, 0, 7, An...|[[token, 0, 7, an...|[[token, 0, 7, an...|[[token, 0, 7, an...|[animalia, illust...|\n",
      "|334|International Ato...|International Ato...|[[document, 0, 67...|[[token, 0, 12, I...|[[token, 0, 12, i...|[[token, 0, 12, i...|[[token, 0, 12, i...|[international, a...|\n",
      "|336|            Altruism|Altruism is the p...|[[document, 0, 32...|[[token, 0, 7, Al...|[[token, 0, 7, al...|[[token, 0, 7, al...|[[token, 0, 7, al...|[altruism, princi...|\n",
      "|339|            Ayn Rand|Ayn Rand (; born ...|[[document, 0, 40...|[[token, 0, 2, Ay...|[[token, 0, 2, ay...|[[token, 0, 2, ay...|[[token, 0, 2, ay...|[ayn, rand, bear,...|\n",
      "|340|        Alain Connes|Alain Connes (; b...|[[document, 0, 98...|[[token, 0, 4, Al...|[[token, 0, 4, al...|[[token, 0, 4, al...|[[token, 0, 4, al...|[alain, connes, b...|\n",
      "|344|          Allan Dwan|Allan Dwan (3 Apr...|[[document, 0, 31...|[[token, 0, 4, Al...|[[token, 0, 4, al...|[[token, 0, 4, al...|[[token, 0, 4, al...|[allan, dwan, apr...|\n",
      "|358|             Algeria|Algeria ( ), offi...|[[document, 0, 61...|[[token, 0, 6, Al...|[[token, 0, 6, al...|[[token, 0, 6, al...|[[token, 0, 6, al...|[algeria, officia...|\n",
      "|359|List of Atlas Shr...|This is a list of...|[[document, 0, 83...|[[token, 0, 3, Th...|[[token, 0, 3, th...|[[token, 0, 3, th...|[[token, 10, 13, ...|[list, character,...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned = pipeline.fit(text).transform(text)\n",
    "cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sub = cleaned.select(['title', 'text'])\n",
    "cleanedRDD = cleaned_sub.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = cleaned.select('finished_clean_lemma')\n",
    "wRDD = w.rdd.flatMap(list)\n",
    "w_list = [x for x in wRDD.toLocalIterator()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRDD = cleaned.select('title').rdd.flatMap(list).distinct()\n",
    "t_list = [x for x in tRDD.toLocalIterator()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedRDD2 = cleanedRDD.map(lambda line:(line[0],list(set(line[1].split(\" \")))))\n",
    "unique_titles = t_list\n",
    "word_ids = cleanedRDD2.flatMap(lambda x:x[1]).distinct().zipWithIndex()\n",
    "word_ids2 = dict([x for x in word_ids.toLocalIterator()])\n",
    "word_ids3 = sc.broadcast(word_ids2)\n",
    "\n",
    "def parseCorpus(line):\n",
    "    A = [(word_ids3.value[el],1) for el in line[1]]\n",
    "    return A\n",
    "\n",
    "corpus = [x for x in cleanedRDD2.map(parseCorpus).toLocalIterator()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models,similarities\n",
    "\n",
    "dictionary = corpora.Dictionary(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"human behavior\"\n",
    "tfidf = models.TfidfModel(corpus) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count = len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_vector = dictionary.doc2bow(keyword.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features = feature_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = index[tfidf[kw_vector]]  # This part overflows the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
